{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning con Scikit-Learn Profesional\n",
    "\n",
    "Scikit-Learn es una biblioteca de Python que ofrece un conjunto de algoritmos eficientes que pueden ser utilizados para realizar Machine Learning en un ambiente productivo. en este articulo Profesional de Machine Learning con SciKit-Learn aprenderás a implementar los principales algoritmos disponibles en esta biblioteca.\n",
    "\n",
    "- Iniciar un proyecto con Scikit-Learn\n",
    "- Aplicar técnicas de regularización a regresiones\n",
    "- Manejar datos atípicos\n",
    "- Reducir la dimensionalidad\n",
    "\n",
    "La herramienta principal será [Scikit Learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "¿Por qué usar Scikit-learn?\n",
    "\n",
    "- Curva de aprendizaje suave.\n",
    "- Es una biblioteca muy versátil.\n",
    "- Comunidad de soporte.\n",
    "- Uso en producción.\n",
    "- Integración con librerías externas.\n",
    "\n",
    "¿Qué podemos hacer con Scikit-learn?\n",
    "\n",
    "- **Clasificación:** Scikit-Learn permite construir modelos de clasificación para asignar etiquetas a nuevas instancias basándose en ejemplos previos.\n",
    "\n",
    "- **Regresión:** Con Scikit-Learn, se pueden aplicar algoritmos de regresión para predecir valores numéricos continuos utilizando variables de entrada.\n",
    "\n",
    "- **Clustering:** Scikit-Learn ofrece algoritmos de clustering para agrupar datos no etiquetados y descubrir patrones y estructuras ocultas.\n",
    "\n",
    "- **Preprocesamiento:** Scikit-Learn proporciona herramientas para el preprocesamiento de datos, incluyendo normalización, estandarización y manejo de valores faltantes, que preparan los datos para el aprendizaje automático.\n",
    "\n",
    "- **Reducción de dimensionalidad:** Scikit-Learn incluye métodos para reducir la cantidad de variables o características en los datos, mejorando la eficiencia y evitando problemas asociados a la dimensionalidad alta.\n",
    "\n",
    "- **Selección del modelo:** Scikit-Learn ofrece herramientas para evaluar y seleccionar modelos de aprendizaje automático, como la validación cruzada y la búsqueda de hiperparámetros, ayudando a elegir el mejor modelo para un problema específico.\n",
    "\n",
    "Preguntas que buscamos responder con Scikit-Learn:\n",
    "\n",
    "- ¿Cómo nos ayuda Scikit-Learn en el preprocesamiento de datos?\n",
    "- ¿Qué modelos podemos utilizar para resolver problemas específicos?\n",
    "- ¿Cuál es el procedimiento a seguir para optimizar los modelos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo aprenden las máquinas?\n",
    "Desde el punto de vista de los datos, podemos aplicar tes tecnicas segun la naturaleza y dsiponibilidad de los mismos:\n",
    "\n",
    "- **Aprendizaje supervisado (Algoritmos por observación):** Utilizamos datos etiquetados para entrenar modelos que puedan predecir o clasificar nuevas instancias.\n",
    "\n",
    "- **Aprendizaje por refuerzo (Algoritmos por prueba y error):** El modelo aprende a través de la interacción con el entorno y la retroalimentación en forma de recompensas o castigos.\n",
    "\n",
    "- **Aprendizaje no supervisado (Algoritmos por descubrimiento):** Se exploran datos no etiquetados para descubrir patrones, estructuras ocultas o grupos similares.\n",
    "\n",
    "\n",
    "El machine learning es solamente una de las posibles ramas que tiene la inteligencia artificial, para otros tipos de problemas existen:\n",
    "\n",
    "\n",
    "- Algoritmos evolutivos: Técnica inspirada en la evolución biológica que busca soluciones óptimas mediante procesos de selección, reproducción y mutación en una población de posibles soluciones.\n",
    "    \n",
    "- Lógica Difusa: Enfoque que permite el razonamiento y la toma de decisiones en situaciones inciertas o imprecisas, utilizando grados de verdad y conjuntos difusos para manejar la ambigüedad.\n",
    "\n",
    "- Agentes: Entidades que perciben su entorno y toman acciones para alcanzar objetivos, utilizados en diversas aplicaciones y pueden emplear diferentes técnicas, como algoritmos de búsqueda, aprendizaje por refuerzo, etc.\n",
    "\n",
    "- Sistemas expertos: Programas informáticos que resuelven problemas en áreas específicas, utilizando conocimiento experto y reglas lógicas para proporcionar recomendaciones o soluciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas que podemos resolver con Scikit-learn\n",
    "\n",
    "- **No es una herramienta de Computer Vision:**  No proporciona funcionalidades avanzadas específicas para el procesamiento de imágenes y tareas de visión por computadora, como detección de objetos, segmentación o reconocimiento facial. Para estas tareas, es mejor utilizar bibliotecas especializadas como OpenCV o frameworks de Deep Learning como TensorFlow, PyTorch o Keras.\n",
    "\n",
    "- **No se puede correr en GPUs:** Si deseas aprovechar la potencia de las GPUs para acelerar tus cálculos en el aprendizaje automático, deberías considerar el uso de bibliotecas y frameworks optimizados para GPU, como TensorFlow con soporte para CUDA o PyTorch con soporte para CUDA.\n",
    "\n",
    "- **No es una herramienta de estadística avanzada:** No ofrece funcionalidades sofisticadas para análisis multivariado, modelado de series de tiempo, análisis de supervivencia o análisis de datos longitudinales. Para estas tareas, es posible que necesites utilizar bibliotecas más especializadas como StatsModels o R (un lenguaje de programación estadística).\n",
    "\n",
    "- **No es muy flexible en temas de Deep Learning:** Si deseas trabajar con modelos de Deep Learning, es mejor considerar el uso de frameworks especializados como TensorFlow, PyTorch o Keras, que brindan una mayor flexibilidad y soporte para arquitecturas de redes neuronales más complejas.\n",
    "\n",
    "### Qué problemas podemos abordar con Scikit-learn?\n",
    "\n",
    "- **Clasificación:** Necesitamos etiquetar nuestros datos para que encajen en alguna de ciertas categorías previamente definidas.\n",
    "\n",
    "    > Ejemplos:\\\n",
    "    > ¿Es cáncer o no es cáncer?\\\n",
    "    > ¿La imagen pertenece a un Ave, Perro o Gato?\\\n",
    "    > ¿A qué segmento de clientes pertenece determinado usuario?\n",
    "\n",
    "- **Regresión:** Cuando necesitamos modelar el comportamiento de una variable continua, dadas otras variables correlacionadas.\n",
    "\n",
    "    > Ejemplos:\\\n",
    "    > Predecir el precio del dólar para el mes siguiente.\\\n",
    "    > El total de calorías de una comida dados sus ingredientes.\\\n",
    "    > La ubicación más probable de determinado objeto dentro de una imagen.\n",
    "\n",
    "- **Clustering:** Queremos descubrir subconjuntos de datos similares dentro del dataset. Queremos encontrar valores que se salen del comportamiento global.\n",
    "\n",
    "    > Ejemplo:\\\n",
    "    > Identificar productos similares para un sistema de recomendación.\\\n",
    "    > Descubrir el sitio ideal para ubicar paradas de autobús según la densidad poblacional.\\\n",
    "    > Segmentar imágenes según patrones de colores y geometrías.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las matemáticas que vamos a necesitar\n",
    "\n",
    "- [Funciones Matemáticas para Data Science e Inteligencia Artificial](https://deepnote.com/@mazzaroli/Introduccion-a-Funciones-Matematicas-para-Data-Science-e-Inteligencia-Artificial-f9a47b52-0308-4e95-a3d3-c3de3ef7b14f)\n",
    "- [Matemáticas para Data Science: Estadística Descriptiva](https://deepnote.com/@mazzaroli/Estadistica-Descriptiva-para-Ciencias-de-Datos-b8622ee2-5fb0-44f3-8791-96915665574e)\n",
    "- [Matemáticas para Ciencias de Datos: Estadística Probabilística](https://deepnote.com/@mazzaroli/Estadistica-Probabilistica-para-Ciencias-de-Datos-aba91c07-8aea-4a1b-9671-3edd06183cf7)\n",
    "- [Matemáticas para Ciencias de Datos: Estadística Inferencial](https://github.com/mazzaroli/inferential-statistics/blob/master/notebook/estadistica-inferencia.ipynb)\n",
    "- [Introducción al Cálculo Diferencial para Data Science e Inteligencia Artificial](https://deepnote.com/@mazzaroli/Calculo-Diferencial-para-Data-Science-e-Inteligencia-Artificial-79fb5f7b-baa0-4ced-b881-b4279275e274)\n",
    "- [Fundamentos de Álgebra Lineal](https://deepnote.com/@mazzaroli/Fundamentos-de-Algebra-Lineal-b2f7b861-a1db-4b8a-9305-654bd16d3900)\n",
    "- [Álgebra Lineal Aplicada para Machine Learning](https://deepnote.com/@mazzaroli/Algebra-Lineal-Aplicada-para-Machine-Learning-9f3a1078-a83d-48b5-b5a6-a67dea878fc8)\n",
    "\n",
    "La estrategia del articulo será seguir el desarrollo de software y ciencia de la computación. Scikit Learn nos ayudará a cubrir algunos vacios conceptuales de una manera que beneficie a nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar un proyecto con sklearn\n",
    "## Configuración de nuestro entorno Python\n",
    "\n",
    "Los entornos virtuales nos permiten isolar multiples dependencias para el desarrollo de proyecto, puede pasar por ejemplo cuando trabajas con diferentes versiones de python o de django.\n",
    "\n",
    "Python 3 trae la creación y manejo de entornos virtuales como parte del modulo central.\n",
    "\n",
    "Entorno virtual con Python\n",
    "\n",
    "Para crear un entorno virtual utilizas:\n",
    "\n",
    "```bash\n",
    "python venv .NOMBRE-ENTORNO\n",
    "```\n",
    "\n",
    "> Nota: .NOMBRE-ENTORNO es el nombre de del ambiente\n",
    "\n",
    "Para activarlo:\n",
    "\n",
    "```bash\n",
    "source -m ./.env/bin/activate\n",
    "```\n",
    "\n",
    "Si queremos desactivarlo:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```\n",
    "Si deseamos ver las librerías instaladas en el ambiente:\n",
    "```bash\n",
    "pip freeze\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de librerías en Python\n",
    "\n",
    "Librerias y sus versiones con las que trabajaremos.\n",
    "\n",
    "Si las copiamos en un archivo requirements.txt y luego con el comando pip install -r requirements.txt podremos instalarlas a todas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets que usaremos en el curso\n",
    "Datasets e informacion sobre ellos en el repo\n",
    "\n",
    "**[World Happiness Report](https://www.kaggle.com/datasets/unsdsn/world-happiness):** Es un dataset que desde el 2012 recolecta variables sobre diferentes países y las relaciona con el nivel de felicidad de sus habitantes.\n",
    "Este data set lo vamos a utilizar para temas de regresiones\n",
    "\n",
    "**[The Ultimate Halloween Candy Power Ranking](https://www.kaggle.com/datasets/fivethirtyeight/the-ultimate-halloween-candy-power-ranking):** Es un estudio online de 269 mil votos de más de 8371 IPs deferentes. Para 85 tipos de dulces diferentes se evaluaron tanto características del dulce como la opinión y satisfacción para generar comparaciones.\n",
    "Este dataset lo vamos a utilizar para temas de clustering\n",
    "\n",
    "**[Heart disease prediction](https://www.kaggle.com/c/SAheart):** Es un subconjunto de variables de un estudio que realizado en 1988 en diferentes regiones del planeta para predecir el riesgo a sufrir una enfermedad relacionada con el corazón.\n",
    "Este data set lo vamos a utilizar para temas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de features\n",
    "\n",
    "## ¿Cómo afectan nuestros features a los modelos de Machine Learning?\n",
    "\n",
    "### ¿Qué son los features? \n",
    "\n",
    "En el contexto de los datos, los **\"features\"** se refieren a las **características o variables** que describen un conjunto de datos: Son las diferentes **columnas o atributos** que contienen información específica sobre cada instancia o muestra en u: conjunto de datos.\n",
    "\n",
    "\n",
    "### ¿Más features es siempre mejor?\n",
    "\n",
    "No necesariamente. Agregar más features puede aumentar la complejidad y el riesgo de sobreajuste del modelo. Es importante seleccionar solo los features relevantes que aporten información útil.\n",
    "\n",
    "- **Features irrelevantes para inferencia:** Los features irrelevantes generan ruido y afectan la precisión del modelo. Es recomendable identificar y eliminar estos features antes de construir el modelo.\n",
    "\n",
    "- **Valores faltantes:** Los valores faltantes en los datos son comunes. Se pueden manejar eliminando muestras con valores faltantes, imputando datos o utilizando algoritmos que los manejen automáticamente.\n",
    "\n",
    "- **Introducción de ruido:** El ruido en los datos se debe a errores y puede afectar los resultados. Es necesario limpiar los datos para eliminar o corregir valores ruidosos.\n",
    "\n",
    "- **Costo computacional:** El procesamiento de grandes volúmenes de datos puede ser costoso. Se pueden reducir costos utilizando técnicas como el muestreo, algoritmos eficientes y optimización del código. Es importante equilibrar el costo computacional con la precisión y utilidad de los resultados.\n",
    "\n",
    "### Sesgo y varianza\n",
    "\n",
    "Una de las formas de saber que nuestros features han sido bien seleccionados es con el **sesgo (bias) y la varianza**, donde se busca encontrar un equilibrio entre sesgo y varianza. Un **sesgo bajo** y una **varianza baja** indican un buen ajuste del modelo a los datos y una capacidad para generalizar correctamente a nuevas instancias. La elección adecuada de features, junto con técnicas de evaluación como la **validación cruzada**, puede ayudar a determinar si el modelo tiene un sesgo y una varianza adecuados.\n",
    "\n",
    "\n",
    "<img src='https://keepcoding.io/wp-content/uploads/2022/12/image-9.png' width=500>\n",
    "\n",
    "<img src='https://keepler.io/wp-content/uploads/2021/03/modelos-machine-learning.png' width=500>\n",
    "\n",
    "### ¿Qué podemos hacer para solucionar estos problemas?\n",
    "\n",
    "- **Feature selection y feature extraction (PCA):** Seleccionar los features relevantes y reducir la dimensionalidad del conjunto de datos para mejorar el rendimiento del modelo.\n",
    "\n",
    "- **Regularización:** Agregar un término de penalización a la función de coste para evitar el sobreajuste y controlar la varianza del modelo.\n",
    "\n",
    "- **Balanceo: Oversampling y Undersampling:** Generar nuevas muestras de la clase minoritaria o reducir la cantidad de muestras de la clase mayoritaria para equilibrar las clases y mejorar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción al PCA\n",
    "\n",
    "### ¿Por qué usaríamos este algoritmo?\n",
    "\n",
    "Porque en machine learning es normal encontrarnos con problemas donde tengamos una **enorme cantidad de features** en donde hay **relaciones complejas** entre ellos y con la variable que queremos predecir.\n",
    "\n",
    "### ¿Dónde se puede utilizar un algoritmo PCA?\n",
    "\n",
    "- Nuestro dataset tiene un número alto de features y no todos son significativos.\n",
    "- Hay una alta correlación entre los features.\n",
    "- Cuando hay sobreajuste.\n",
    "- Cuando implica un alto costo computacional.\n",
    "\n",
    "### ¿En que consiste el algoritmo PCA?\n",
    "\n",
    "La tecnica PCA consiste en reducir la complejidad de los features:\n",
    "\n",
    "- Seleccionando solamente las variables relevantes.\n",
    "- Combinándolas en nuevas variables que mantengan la información más importante (varianza de los features).\n",
    "\n",
    "<img src='https://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg' width='450'>\n",
    "\n",
    "### ¿Cuales son pasos para llevar a cabo el algoritmo PCA?\n",
    "\n",
    "- Calculamos la matriz de covarianza para expresar las relaciones entre nuestro features.\n",
    "- Hallamos los vectores propios y valores propios de esta matriz, para medir la fuerza y variabilidad de estas relaciones.\n",
    "- Ordenamos y escogemos los vectores propios con mayor variabilidad, esto es, aportan más información.\n",
    "\n",
    "[ANÁLISIS DE COMPONENTES PRINCIPALES (PCA)](https://www.youtube.com/watch?v=k3CWA2GBb8o)\n",
    "\n",
    "### ¿Qué hacer si tenemos una PC de bajos recursos?\n",
    "\n",
    "- Si tenemos un dataset demasiado exigente, podemos usar una variación como IPCA.\n",
    "- Si nuestros datos no tienen una estructura separable linealmente, y encontramos un KERNEL que pueda mapearlos podemos usar KPCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para PCA e IPCA\n",
    "\n",
    "A partir de ahora, crearemos un archivo llamado pca.py en el que llevaremos a cabo los siguientes pasos:\n",
    "\n",
    "En el código proporcionado, se realizan los siguientes pasos:\n",
    "\n",
    "1. Se importan las bibliotecas necesarias para el manejo de datos estructurados, machine learning y visualización de gráficos.\n",
    "\n",
    "2. Se importan las clases específicas de scikit-learn para realizar análisis de componentes principales, regresión logística, estandarización de características y división de los datos en entrenamiento y prueba.\n",
    "\n",
    "3. Se carga un conjunto de datos del archivo `heart.csv` utilizando la biblioteca pandas y se muestra una vista previa de los primeros 5 registros.\n",
    "\n",
    "4. Se separan los features (características) y la variable objetivo del conjunto de datos.\n",
    "\n",
    "5. Se estandarizan las características utilizando la clase `StandardScaler` de scikit-learn para asegurar que tengan una escala común.\n",
    "\n",
    "6. Se divide el conjunto de datos en conjuntos de entrenamiento y prueba utilizando la función `train_test_split` de scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del algoritmo PCA e IPCA\n",
    "\n",
    "1. Se realiza el análisis de componentes principales (PCA) y el análisis de componentes principales incremental (IPCA) utilizando los datos de entrenamiento.\n",
    "\n",
    "1. Se grafica la varianza explicada por cada componente principal utilizando Matplotlib y se guarda el gráfico en un archivo llamado 'a'.\n",
    "\n",
    "1. Se crea un clasificador de regresión logística.\n",
    "\n",
    "1. Se transforman los datos de entrenamiento y prueba utilizando PCA y se ajusta el modelo de regresión logística utilizando los datos transformados con PCA. Se imprime el score del modelo utilizando los datos de prueba transformados con PCA.\n",
    "\n",
    "1. Se transforman los datos de entrenamiento y prueba utilizando IPCA y se ajusta el modelo de regresión logística utilizando los datos transformados con IPCA. Se imprime el score del modelo utilizando los datos de prueba transformados con IPCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels y KPCA\n",
    "\n",
    "Una alternativa son los **Kernels**. Un **Kernel** es una función matemática que toma mediciones que se comportan de manera no lineal y las proyecta en un espacio dimensional más grande en donde son **linealmente separables**.\n",
    "\n",
    "### ¿Para qué sirven los kernels?\n",
    "\n",
    "Los **kernels** son utilizados en el aprendizaje automático para **transformar datos** a un espacio dimensional más grande, donde pueden ser más fácilmente **separables**. Esto es útil cuando los datos no pueden ser separados linealmente en su espacio original. Los kernels permiten aplicar algoritmos de clasificación o regresión lineal en un espacio de mayor dimensión sin la necesidad de una transformación explícita. Son comúnmente utilizados en algoritmos como las **Máquinas de Vectores de Soporte (SVM)** para lograr una separación óptima de las clases.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:838/0*UYg7pGkmsjI5ArZR.png' width=500>\n",
    "\n",
    "Aquí tienes una breve descripción de los 3 tipos de kernels comunes y sus fórmulas:\n",
    "\n",
    "1. **Kernel lineal**: El kernel lineal es el más simple y se utiliza para problemas de clasificación o regresión lineal. Su fórmula es simplemente el producto escalar entre dos vectores:\n",
    "\n",
    "   $\\displaystyle K(x, y) = x^T * y$\n",
    "\n",
    "2. **Kernel polinomial**: El kernel polinomial se utiliza para mapear los datos a un espacio dimensional más alto utilizando una función polinomial. La fórmula general del kernel polinomial es:\n",
    "\n",
    "   $\\displaystyle K(x, y) = (α * x^T * y + c)^d$\n",
    "\n",
    "   Donde α es un coeficiente, c es una constante y d es el grado del polinomio.\n",
    "\n",
    "3. **Kernel gaussiano (RBF)**: El kernel gaussiano, también conocido como kernel RBF (Radial Basis Function), es utilizado para mapear los datos a un espacio dimensional infinito. Su fórmula es:\n",
    "\n",
    "   $\\displaystyle K(x, y) = e - \\frac{||x - y||^2}{2\\sigma^2} $\n",
    "\n",
    "   Donde:\n",
    "\n",
    "   - $K(x, y)$ representa el valor del kernel entre dos puntos x e y.\n",
    "   - $||x - y||^2$ es la distancia euclidiana al cuadrado entre los puntos x e y.\n",
    "   - $σ$ (sigma) es un parámetro que controla la amplitud del kernel y la influencia de cada muestra vecina en la clasificación final.\n",
    "\n",
    "<img src='https://qu4nt.github.io/sklearn-doc-es/_images/sphx_glr_plot_iris_svc_001.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es la regularización y cómo aplicarla?\n",
    "\n",
    "La **regularización** es una técnica importante en el aprendizaje automático que nos permite reducir la complejidad de nuestro modelo y evitar el sobreajuste. Consiste en aplicar una penalización a las variables menos relevantes del modelo.\n",
    "\n",
    "**¿Como hacemos lo anterior?**\n",
    "\n",
    "Para lograr esto, introducimos un **mayor sesgo** sobre las variables y disminuimos la varianza. De esta manera, mejoramos la capacidad de generalización del modelo y evitamos o reducimos el sobreajuste.\n",
    "\n",
    "<img src='https://www.andreaperlato.com/img/ridge.png'>\n",
    "\n",
    "En la imagen proporcionada, se muestra un ejemplo de sobreajuste. La línea azul se ajusta muy bien a los datos de prueba, pero no a los datos de entrenamiento. Esto resulta en una mala generalización y una aproximación deficiente.\n",
    "\n",
    "Para aplicar la regularización, necesitamos incluir un término adicional conocido como **función de pérdida (loss)**. La función de pérdida nos indica qué tan lejos están nuestras predicciones de los datos reales. En general, cuanto menor sea la pérdida, mejor será nuestro modelo.\n",
    "\n",
    "### Perdida en entrenamiento y en validación\n",
    "\n",
    "<img src='https://developers.google.com/static/machine-learning/crash-course/images/RegularizationTwoLossFunctions.svg' width=500>\n",
    "\n",
    "Podemos ver en la gráfica que la **pérdida tiende a disminuir**, porque en algún momento los datos de entrenamiento serán vistos y el modelo se ajustará a ellos. Sin embargo, lo importante es **cómo se comportará en el mundo real**.\n",
    "\n",
    "En el conjunto de validación o pruebas, es **normal que la pérdida comience a disminuir**, ya que indica una buena generalización. Sin embargo, llega un punto donde la introducción de nuevos valores hace que la pérdida vuelva a subir, lo cual generalmente se considera como **sobreajuste**. La pérdida es la medida que utilizamos para **aplicar la regularización**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tipos de regularización\n",
    "\n",
    "- **L1 (Lasso)**: Este regulizador reduce la complejidad del modelo eliminando características que tienen un impacto mínimo en la predicción. Al aplicar la regularización L1, algunos coeficientes del modelo se vuelven exactamente cero, lo que implica que estas características no son consideradas en la predicción final. Es útil para la selección automática de características relevantes.\n",
    "\n",
    "    $\\displaystyle\\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^{n}  \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} |\\beta|}$\n",
    "\n",
    "\n",
    "    Aquí está la descripción de cada componente de la fórmula:\n",
    "\n",
    "    - $\\displaystyle\\underset{\\beta}{\\text{arg min}}$: Se refiere al valor de los coeficientes $\\beta$ que minimiza la función de costo.\n",
    "\n",
    "    - $\\sum_{i=1}^{n}$: Representa la suma de los índices $i$ desde 1 hasta $n$, donde $n$ es el número de observaciones en el conjunto de datos.\n",
    "\n",
    "    - $y_i$: Es el valor real de la variable objetivo para la observación $i$.\n",
    "\n",
    "    - $\\beta_0$: Es el coeficiente de intersección o término de sesgo.\n",
    "\n",
    "    - $\\sum_{j=1}^{p}$: Representa la suma de los índices $j$ desde 1 hasta $p$, donde $p$ es el número de características o variables independientes en el conjunto de datos.\n",
    "\n",
    "    - $\\beta_j$: Son los coeficientes asociados a las características $x_{ij}$ del modelo.\n",
    "\n",
    "    - $x_{ij}$: Representa el valor de la característica $j$ para la observación $i$.\n",
    "\n",
    "    - $\\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} |\\beta|}$: Es la penalización que se aplica a los coeficientes. Se utiliza la norma L1 (valor absoluto) para sumar los coeficientes $\\beta_j$. El parámetro $\\lambda$ controla la intensidad de la penalización, donde un valor más alto de $\\lambda$ lleva a una mayor reducción de los coeficientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **L2 (Ridge)**: El regulizador L2 reduce la complejidad del modelo disminuyendo el impacto de ciertos features en la predicción. Penaliza los coeficientes grandes y favorece coeficientes más pequeños. A diferencia de L1, L2 no descarta características, sino que las reduce gradualmente. Es útil para evitar la multicolinealidad y mejorar la estabilidad del modelo.\n",
    "\n",
    "    $\\displaystyle\\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^{n}  \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} \\beta^2_{j}}$\n",
    "\n",
    "    Teniendo en cuenta que la descripción del regulador L2 (Ridge) contiene algunos puntos que se repiten con la descripción del regulador L1 (Lasso), los puntos que se pueden omitir en la descripción del regulador L2 son los siguientes:\n",
    "\n",
    "    - El uso de la función de costo cuadrática $(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2$ en lugar de la función de costo absoluta en la descripción del Lasso.\n",
    "\n",
    "    - $\\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} \\beta^2_{j}}$: Es la penalización que se aplica a los coeficientes. Se utiliza la norma L2 (cuadrados) para sumar los coeficientes $\\beta_j$ al cuadrado. El parámetro $\\lambda$ controla la intensidad de la penalización, donde un valor más alto de $\\lambda$ lleva a una mayor reducción de los coeficientes.\n",
    "\n",
    "- **ElasticNet**: Es una combinación de los regulizadores L1 y L2. Combina los efectos de selección automática de características de L1 con la regularización de coeficientes de L2. Proporciona un equilibrio entre la eliminación de características irrelevantes y la reducción del impacto de ciertos features en el modelo. Es útil cuando se sospecha que hay muchas características irrelevantes en los datos.\n",
    "\n",
    "   $\\displaystyle\\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^{n}  \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 +  \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2$\n",
    "\n",
    "   Donde:\n",
    "\n",
    "    - $(\\lambda_1)$ y $(\\lambda_2)$ son hiperparámetros que controlan la intensidad de las penalizaciones L1 y L2, respectivamente.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Lasso y Ridge\n",
    "\n",
    "Implementaremos las tecnicas de regularizacion. Para esto utilizaremos dos regresores que vienen por defecto en Scikit Learn y que de una manera automatizada nos integra un modelo lineal con su respectiva regularización.\n",
    "\n",
    "Utilizaremos el dataset del reporte de la felicidad mundial. Dataset que mide varios factores en diferentes paises, tales como, el indice de corrupción, nivel que nos indica que tan fuerte son las relaciones de familia, el desarrollo per capita económico y nos intenta dar una variable continua para medir la felicidad del pais en cuestión.\n",
    "\n",
    "Pasamos a crear un archivo con nombre ```regularization.py```\n",
    "\n",
    "1. Importar las bibliotecas necesarias:\n",
    "   - Se importa `pandas` para el manejo de datos estructurados.\n",
    "   - Se importan clases específicas de los módulos `sklearn.linear_model`, `sklearn.metrics` y `sklearn.model_selection` para su uso posterior.\n",
    "\n",
    "2. Leer el conjunto de datos:\n",
    "   - Se utiliza la función `pd.read_csv()` para leer un archivo CSV que contiene los datos del conjunto de datos.\n",
    "\n",
    "3. Separar características y variable objetivo:\n",
    "   - Se seleccionan las columnas específicas del conjunto de datos que se utilizarán como características (`X`) y se asignan a la variable `X`.\n",
    "   - Se selecciona la columna que se utilizará como la variable objetivo (`y`) y se asigna a la variable `y`.\n",
    "\n",
    "4. Dividir el conjunto de datos en conjuntos de entrenamiento y prueba:\n",
    "   - Se utiliza la función `train_test_split()` del módulo `sklearn.model_selection` para dividir los datos en conjuntos de entrenamiento (`X_train`, `y_train`) y prueba (`X_test`, `y_test`). Se especifica que el 25% de los datos se utilizarán como conjunto de prueba y se establece una semilla aleatoria (`random_state`) para la reproducibilidad.\n",
    "\n",
    "5. Entrenar los modelos de regresión:\n",
    "   - Se crean instancias de los modelos `LinearRegression`, `Lasso`, `Ridge` y `ElasticNet` del módulo `sklearn.linear_model`.\n",
    "   - Se ajustan (`fit()`) los modelos a los conjuntos de entrenamiento (`X_train`, `y_train`).\n",
    "\n",
    "6. Realizar predicciones en el conjunto de prueba:\n",
    "   - Se utilizan los modelos entrenados para hacer predicciones (`predict()`) en los conjuntos de prueba (`X_test`) y se guardan en las variables correspondientes (`y_predict_lineal`, `y_predict_lasso`, `y_predict_ridge`, `y_predict_elastic`).\n",
    "\n",
    "7. Calcular el MSE para cada modelo:\n",
    "   - Se utiliza la función `mean_squared_error()` del módulo `sklearn.metrics` para calcular el error cuadrático medio (MSE) entre las predicciones y los valores reales de las variables objetivo (`y_test`). Se calcula un MSE para cada modelo y se guarda en las variables correspondientes (`lineal_loss`, `lasso_loss`, `ridge_loss`, `elastic_loss`).\n",
    "\n",
    "8. Imprimir los valores de MSE para cada modelo:\n",
    "   - Se imprimen los valores de MSE calculados para cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación resultado de la implementación\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
