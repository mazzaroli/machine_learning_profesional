{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning con Scikit-Learn Profesional\n",
    "\n",
    "Scikit-Learn es una biblioteca de Python que ofrece un conjunto de algoritmos eficientes que pueden ser utilizados para realizar Machine Learning en un ambiente productivo. en este articulo Profesional de Machine Learning con SciKit-Learn aprenderás a implementar los principales algoritmos disponibles en esta biblioteca.\n",
    "\n",
    "- Iniciar un proyecto con Scikit-Learn\n",
    "- Aplicar técnicas de regularización a regresiones\n",
    "- Manejar datos atípicos\n",
    "- Reducir la dimensionalidad\n",
    "\n",
    "La herramienta principal será [Scikit Learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "¿Por qué usar Scikit-learn?\n",
    "\n",
    "- Curva de aprendizaje suave.\n",
    "- Es una biblioteca muy versátil.\n",
    "- Comunidad de soporte.\n",
    "- Uso en producción.\n",
    "- Integración con librerías externas.\n",
    "\n",
    "¿Qué podemos hacer con Scikit-learn?\n",
    "\n",
    "- **Clasificación:** Scikit-Learn permite construir modelos de clasificación para asignar etiquetas a nuevas instancias basándose en ejemplos previos.\n",
    "\n",
    "- **Regresión:** Con Scikit-Learn, se pueden aplicar algoritmos de regresión para predecir valores numéricos continuos utilizando variables de entrada.\n",
    "\n",
    "- **Clustering:** Scikit-Learn ofrece algoritmos de clustering para agrupar datos no etiquetados y descubrir patrones y estructuras ocultas.\n",
    "\n",
    "- **Preprocesamiento:** Scikit-Learn proporciona herramientas para el preprocesamiento de datos, incluyendo normalización, estandarización y manejo de valores faltantes, que preparan los datos para el aprendizaje automático.\n",
    "\n",
    "- **Reducción de dimensionalidad:** Scikit-Learn incluye métodos para reducir la cantidad de variables o características en los datos, mejorando la eficiencia y evitando problemas asociados a la dimensionalidad alta.\n",
    "\n",
    "- **Selección del modelo:** Scikit-Learn ofrece herramientas para evaluar y seleccionar modelos de aprendizaje automático, como la validación cruzada y la búsqueda de hiperparámetros, ayudando a elegir el mejor modelo para un problema específico.\n",
    "\n",
    "Preguntas que buscamos responder con Scikit-Learn:\n",
    "\n",
    "- ¿Cómo nos ayuda Scikit-Learn en el preprocesamiento de datos?\n",
    "- ¿Qué modelos podemos utilizar para resolver problemas específicos?\n",
    "- ¿Cuál es el procedimiento a seguir para optimizar los modelos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo aprenden las máquinas?\n",
    "Desde el punto de vista de los datos, podemos aplicar tes tecnicas segun la naturaleza y dsiponibilidad de los mismos:\n",
    "\n",
    "- **Aprendizaje supervisado (Algoritmos por observación):** Utilizamos datos etiquetados para entrenar modelos que puedan predecir o clasificar nuevas instancias.\n",
    "\n",
    "- **Aprendizaje por refuerzo (Algoritmos por prueba y error):** El modelo aprende a través de la interacción con el entorno y la retroalimentación en forma de recompensas o castigos.\n",
    "\n",
    "- **Aprendizaje no supervisado (Algoritmos por descubrimiento):** Se exploran datos no etiquetados para descubrir patrones, estructuras ocultas o grupos similares.\n",
    "\n",
    "\n",
    "El machine learning es solamente una de las posibles ramas que tiene la inteligencia artificial, para otros tipos de problemas existen:\n",
    "\n",
    "\n",
    "- Algoritmos evolutivos: Técnica inspirada en la evolución biológica que busca soluciones óptimas mediante procesos de selección, reproducción y mutación en una población de posibles soluciones.\n",
    "    \n",
    "- Lógica Difusa: Enfoque que permite el razonamiento y la toma de decisiones en situaciones inciertas o imprecisas, utilizando grados de verdad y conjuntos difusos para manejar la ambigüedad.\n",
    "\n",
    "- Agentes: Entidades que perciben su entorno y toman acciones para alcanzar objetivos, utilizados en diversas aplicaciones y pueden emplear diferentes técnicas, como algoritmos de búsqueda, aprendizaje por refuerzo, etc.\n",
    "\n",
    "- Sistemas expertos: Programas informáticos que resuelven problemas en áreas específicas, utilizando conocimiento experto y reglas lógicas para proporcionar recomendaciones o soluciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas que podemos resolver con Scikit-learn\n",
    "\n",
    "- **No es una herramienta de Computer Vision:**  No proporciona funcionalidades avanzadas específicas para el procesamiento de imágenes y tareas de visión por computadora, como detección de objetos, segmentación o reconocimiento facial. Para estas tareas, es mejor utilizar bibliotecas especializadas como OpenCV o frameworks de Deep Learning como TensorFlow, PyTorch o Keras.\n",
    "\n",
    "- **No se puede correr en GPUs:** Si deseas aprovechar la potencia de las GPUs para acelerar tus cálculos en el aprendizaje automático, deberías considerar el uso de bibliotecas y frameworks optimizados para GPU, como TensorFlow con soporte para CUDA o PyTorch con soporte para CUDA.\n",
    "\n",
    "- **No es una herramienta de estadística avanzada:** No ofrece funcionalidades sofisticadas para análisis multivariado, modelado de series de tiempo, análisis de supervivencia o análisis de datos longitudinales. Para estas tareas, es posible que necesites utilizar bibliotecas más especializadas como StatsModels o R (un lenguaje de programación estadística).\n",
    "\n",
    "- **No es muy flexible en temas de Deep Learning:** Si deseas trabajar con modelos de Deep Learning, es mejor considerar el uso de frameworks especializados como TensorFlow, PyTorch o Keras, que brindan una mayor flexibilidad y soporte para arquitecturas de redes neuronales más complejas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Qué problemas podemos abordar con Scikit-learn?\n",
    "\n",
    "- **Clasificación:** Necesitamos etiquetar nuestros datos para que encajen en alguna de ciertas categorías previamente definidas.\n",
    "\n",
    "    > Ejemplos:\\\n",
    "    > ¿Es cáncer o no es cáncer?\\\n",
    "    > ¿La imagen pertenece a un Ave, Perro o Gato?\\\n",
    "    > ¿A qué segmento de clientes pertenece determinado usuario?\n",
    "\n",
    "- **Regresión:** Cuando necesitamos modelar el comportamiento de una variable continua, dadas otras variables correlacionadas.\n",
    "\n",
    "    > Ejemplos:\\\n",
    "    > Predecir el precio del dólar para el mes siguiente.\\\n",
    "    > El total de calorías de una comida dados sus ingredientes.\\\n",
    "    > La ubicación más probable de determinado objeto dentro de una imagen.\n",
    "\n",
    "- **Clustering:** Queremos descubrir subconjuntos de datos similares dentro del dataset. Queremos encontrar valores que se salen del comportamiento global.\n",
    "\n",
    "    > Ejemplo:\\\n",
    "    > Identificar productos similares para un sistema de recomendación.\\\n",
    "    > Descubrir el sitio ideal para ubicar paradas de autobús según la densidad poblacional.\\\n",
    "    > Segmentar imágenes según patrones de colores y geometrías.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las matemáticas que vamos a necesitar\n",
    "\n",
    "- [Funciones Matemáticas para Data Science e Inteligencia Artificial](https://deepnote.com/@mazzaroli/Introduccion-a-Funciones-Matematicas-para-Data-Science-e-Inteligencia-Artificial-f9a47b52-0308-4e95-a3d3-c3de3ef7b14f)\n",
    "- [Matemáticas para Data Science: Estadística Descriptiva](https://deepnote.com/@mazzaroli/Estadistica-Descriptiva-para-Ciencias-de-Datos-b8622ee2-5fb0-44f3-8791-96915665574e)\n",
    "- [Matemáticas para Ciencias de Datos: Estadística Probabilística](https://deepnote.com/@mazzaroli/Estadistica-Probabilistica-para-Ciencias-de-Datos-aba91c07-8aea-4a1b-9671-3edd06183cf7)\n",
    "- [Matemáticas para Ciencias de Datos: Estadística Inferencial](https://github.com/mazzaroli/inferential-statistics/blob/master/notebook/estadistica-inferencia.ipynb)\n",
    "- [Introducción al Cálculo Diferencial para Data Science e Inteligencia Artificial](https://deepnote.com/@mazzaroli/Calculo-Diferencial-para-Data-Science-e-Inteligencia-Artificial-79fb5f7b-baa0-4ced-b881-b4279275e274)\n",
    "- [Fundamentos de Álgebra Lineal](https://deepnote.com/@mazzaroli/Fundamentos-de-Algebra-Lineal-b2f7b861-a1db-4b8a-9305-654bd16d3900)\n",
    "- [Álgebra Lineal Aplicada para Machine Learning](https://deepnote.com/@mazzaroli/Algebra-Lineal-Aplicada-para-Machine-Learning-9f3a1078-a83d-48b5-b5a6-a67dea878fc8)\n",
    "\n",
    "La estrategia del articulo será seguir el desarrollo de software y ciencia de la computación. Scikit Learn nos ayudará a cubrir algunos vacios conceptuales de una manera que beneficie a nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciar un proyecto con sklearn\n",
    "\n",
    "## Configuración de nuestro entorno Python\n",
    "\n",
    "Los entornos virtuales nos permiten isolar multiples dependencias para el desarrollo de proyecto, puede pasar por ejemplo cuando trabajas con diferentes versiones de python o de django.\n",
    "\n",
    "Python 3 trae la creación y manejo de entornos virtuales como parte del modulo central.\n",
    "\n",
    "Entorno virtual con Python\n",
    "\n",
    "Para crear un entorno virtual utilizas:\n",
    "\n",
    "```bash\n",
    "python venv .NOMBRE-ENTORNO\n",
    "```\n",
    "\n",
    "> Nota: .NOMBRE-ENTORNO es el nombre de del ambiente\n",
    "\n",
    "Para activarlo:\n",
    "\n",
    "```bash\n",
    "source -m ./.env/bin/activate\n",
    "```\n",
    "\n",
    "Si queremos desactivarlo:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```\n",
    "Si deseamos ver las librerías instaladas en el ambiente:\n",
    "```bash\n",
    "pip freeze\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de librerías en Python\n",
    "\n",
    "Librerias y sus versiones con las que trabajaremos.\n",
    "\n",
    "Si las copiamos en un archivo requirements.txt y luego con el comando pip install -r requirements.txt podremos instalarlas a todas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets que usaremos en el curso\n",
    "Datasets e informacion sobre ellos en el repo\n",
    "\n",
    "**[World Happiness Report](https://www.kaggle.com/datasets/unsdsn/world-happiness):** Es un dataset que desde el 2012 recolecta variables sobre diferentes países y las relaciona con el nivel de felicidad de sus habitantes.\n",
    "Este data set lo vamos a utilizar para temas de regresiones\n",
    "\n",
    "**[The Ultimate Halloween Candy Power Ranking](https://www.kaggle.com/datasets/fivethirtyeight/the-ultimate-halloween-candy-power-ranking):** Es un estudio online de 269 mil votos de más de 8371 IPs deferentes. Para 85 tipos de dulces diferentes se evaluaron tanto características del dulce como la opinión y satisfacción para generar comparaciones.\n",
    "Este dataset lo vamos a utilizar para temas de clustering\n",
    "\n",
    "**[Heart disease prediction](https://www.kaggle.com/c/SAheart):** Es un subconjunto de variables de un estudio que realizado en 1988 en diferentes regiones del planeta para predecir el riesgo a sufrir una enfermedad relacionada con el corazón.\n",
    "Este data set lo vamos a utilizar para temas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de features\n",
    "\n",
    "## ¿Cómo afectan nuestros features a los modelos de Machine Learning?\n",
    "\n",
    "### ¿Qué son los features? \n",
    "\n",
    "En el contexto de los datos, los **\"features\"** se refieren a las **características o variables** que describen un conjunto de datos: Son las diferentes **columnas o atributos** que contienen información específica sobre cada instancia o muestra en u: conjunto de datos.\n",
    "\n",
    "\n",
    "### ¿Más features es siempre mejor?\n",
    "\n",
    "No necesariamente. Agregar más features puede aumentar la complejidad y el riesgo de sobreajuste del modelo. Es importante seleccionar solo los features relevantes que aporten información útil.\n",
    "\n",
    "- **Features irrelevantes para inferencia:** Los features irrelevantes generan ruido y afectan la precisión del modelo. Es recomendable identificar y eliminar estos features antes de construir el modelo.\n",
    "\n",
    "- **Valores faltantes:** Los valores faltantes en los datos son comunes. Se pueden manejar eliminando muestras con valores faltantes, imputando datos o utilizando algoritmos que los manejen automáticamente.\n",
    "\n",
    "- **Introducción de ruido:** El ruido en los datos se debe a errores y puede afectar los resultados. Es necesario limpiar los datos para eliminar o corregir valores ruidosos.\n",
    "\n",
    "- **Costo computacional:** El procesamiento de grandes volúmenes de datos puede ser costoso. Se pueden reducir costos utilizando técnicas como el muestreo, algoritmos eficientes y optimización del código. Es importante equilibrar el costo computacional con la precisión y utilidad de los resultados.\n",
    "\n",
    "### Sesgo y varianza\n",
    "\n",
    "Una de las formas de saber que nuestros features han sido bien seleccionados es con el **sesgo (bias) y la varianza**, donde se busca encontrar un equilibrio entre sesgo y varianza. Un **sesgo bajo** y una **varianza baja** indican un buen ajuste del modelo a los datos y una capacidad para generalizar correctamente a nuevas instancias. La elección adecuada de features, junto con técnicas de evaluación como la **validación cruzada**, puede ayudar a determinar si el modelo tiene un sesgo y una varianza adecuados.\n",
    "\n",
    "\n",
    "<img src='https://keepcoding.io/wp-content/uploads/2022/12/image-9.png' width=500>\n",
    "\n",
    "<img src='https://keepler.io/wp-content/uploads/2021/03/modelos-machine-learning.png' width=500>\n",
    "\n",
    "### ¿Qué podemos hacer para solucionar estos problemas?\n",
    "\n",
    "- **Feature selection y feature extraction (PCA):** Seleccionar los features relevantes y reducir la dimensionalidad del conjunto de datos para mejorar el rendimiento del modelo.\n",
    "\n",
    "- **Regularización:** Agregar un término de penalización a la función de coste para evitar el sobreajuste y controlar la varianza del modelo.\n",
    "\n",
    "- **Balanceo: Oversampling y Undersampling:** Generar nuevas muestras de la clase minoritaria o reducir la cantidad de muestras de la clase mayoritaria para equilibrar las clases y mejorar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción al PCA\n",
    "\n",
    "### ¿Por qué usaríamos este algoritmo?\n",
    "\n",
    "Porque en machine learning es normal encontrarnos con problemas donde tengamos una **enorme cantidad de features** en donde hay **relaciones complejas** entre ellos y con la variable que queremos predecir.\n",
    "\n",
    "### ¿Dónde se puede utilizar un algoritmo PCA?\n",
    "\n",
    "- Nuestro dataset tiene un número alto de features y no todos son significativos.\n",
    "- Hay una alta correlación entre los features.\n",
    "- Cuando hay sobreajuste.\n",
    "- Cuando implica un alto costo computacional.\n",
    "\n",
    "### ¿En que consiste el algoritmo PCA?\n",
    "\n",
    "La tecnica PCA consiste en reducir la complejidad de los features:\n",
    "\n",
    "- Seleccionando solamente las variables relevantes.\n",
    "- Combinándolas en nuevas variables que mantengan la información más importante (varianza de los features).\n",
    "\n",
    "<img src='https://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg' width='450'>\n",
    "\n",
    "### ¿Cuales son pasos para llevar a cabo el algoritmo PCA?\n",
    "\n",
    "- Calculamos la matriz de covarianza para expresar las relaciones entre nuestro features.\n",
    "- Hallamos los vectores propios y valores propios de esta matriz, para medir la fuerza y variabilidad de estas relaciones.\n",
    "- Ordenamos y escogemos los vectores propios con mayor variabilidad, esto es, aportan más información.\n",
    "\n",
    "[ANÁLISIS DE COMPONENTES PRINCIPALES (PCA)](https://www.youtube.com/watch?v=k3CWA2GBb8o)\n",
    "\n",
    "### ¿Qué hacer si tenemos una PC de bajos recursos?\n",
    "\n",
    "- Si tenemos un dataset demasiado exigente, podemos usar una variación como IPCA.\n",
    "- Si nuestros datos no tienen una estructura separable linealmente, y encontramos un KERNEL que pueda mapearlos podemos usar KPCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para PCA e IPCA\n",
    "\n",
    "A partir de ahora, crearemos un archivo llamado pca.py en el que llevaremos a cabo los siguientes pasos:\n",
    "\n",
    "En el código proporcionado, se realizan los siguientes pasos:\n",
    "\n",
    "1. Se importan las bibliotecas necesarias para el manejo de datos estructurados, machine learning y visualización de gráficos.\n",
    "\n",
    "2. Se importan las clases específicas de scikit-learn para realizar análisis de componentes principales, regresión logística, estandarización de características y división de los datos en entrenamiento y prueba.\n",
    "\n",
    "3. Se carga un conjunto de datos del archivo `heart.csv` utilizando la biblioteca pandas y se muestra una vista previa de los primeros 5 registros.\n",
    "\n",
    "4. Se separan los features (características) y la variable objetivo del conjunto de datos.\n",
    "\n",
    "5. Se estandarizan las características utilizando la clase `StandardScaler` de scikit-learn para asegurar que tengan una escala común.\n",
    "\n",
    "6. Se divide el conjunto de datos en conjuntos de entrenamiento y prueba utilizando la función `train_test_split` de scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del algoritmo PCA e IPCA\n",
    "\n",
    "1. Se realiza el análisis de componentes principales (PCA) y el análisis de componentes principales incremental (IPCA) utilizando los datos de entrenamiento.\n",
    "\n",
    "1. Se grafica la varianza explicada por cada componente principal utilizando Matplotlib y se guarda el gráfico en un archivo llamado 'a'.\n",
    "\n",
    "1. Se crea un clasificador de regresión logística.\n",
    "\n",
    "1. Se transforman los datos de entrenamiento y prueba utilizando PCA y se ajusta el modelo de regresión logística utilizando los datos transformados con PCA. Se imprime el score del modelo utilizando los datos de prueba transformados con PCA.\n",
    "\n",
    "1. Se transforman los datos de entrenamiento y prueba utilizando IPCA y se ajusta el modelo de regresión logística utilizando los datos transformados con IPCA. Se imprime el score del modelo utilizando los datos de prueba transformados con IPCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels y KPCA\n",
    "\n",
    "Una alternativa son los **Kernels**. Un **Kernel** es una función matemática que toma mediciones que se comportan de manera no lineal y las proyecta en un espacio dimensional más grande en donde son **linealmente separables**.\n",
    "\n",
    "### ¿Para qué sirven los kernels?\n",
    "\n",
    "Los **kernels** son utilizados en el aprendizaje automático para **transformar datos** a un espacio dimensional más grande, donde pueden ser más fácilmente **separables**. Esto es útil cuando los datos no pueden ser separados linealmente en su espacio original. Los kernels permiten aplicar algoritmos de clasificación o regresión lineal en un espacio de mayor dimensión sin la necesidad de una transformación explícita. Son comúnmente utilizados en algoritmos como las **Máquinas de Vectores de Soporte (SVM)** para lograr una separación óptima de las clases.\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:838/0*UYg7pGkmsjI5ArZR.png' width=500>\n",
    "\n",
    "Aquí tienes una breve descripción de los 3 tipos de kernels comunes y sus fórmulas:\n",
    "\n",
    "1. **Kernel lineal**: El kernel lineal es el más simple y se utiliza para problemas de clasificación o regresión lineal. Su fórmula es simplemente el producto escalar entre dos vectores:\n",
    "\n",
    "   $\\displaystyle K(x, y) = x^T * y$\n",
    "\n",
    "2. **Kernel polinomial**: El kernel polinomial se utiliza para mapear los datos a un espacio dimensional más alto utilizando una función polinomial. La fórmula general del kernel polinomial es:\n",
    "\n",
    "   $\\displaystyle K(x, y) = (α * x^T * y + c)^d$\n",
    "\n",
    "   Donde α es un coeficiente, c es una constante y d es el grado del polinomio.\n",
    "\n",
    "3. **Kernel gaussiano (RBF)**: El kernel gaussiano, también conocido como kernel RBF (Radial Basis Function), es utilizado para mapear los datos a un espacio dimensional infinito. Su fórmula es:\n",
    "\n",
    "   $\\displaystyle K(x, y) = e - \\frac{||x - y||^2}{2\\sigma^2} $\n",
    "\n",
    "   Donde:\n",
    "\n",
    "   - $K(x, y)$ representa el valor del kernel entre dos puntos x e y.\n",
    "   - $||x - y||^2$ es la distancia euclidiana al cuadrado entre los puntos x e y.\n",
    "   - $σ$ (sigma) es un parámetro que controla la amplitud del kernel y la influencia de cada muestra vecina en la clasificación final.\n",
    "\n",
    "<img src='https://qu4nt.github.io/sklearn-doc-es/_images/sphx_glr_plot_iris_svc_001.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es la regularización y cómo aplicarla?\n",
    "\n",
    "La **regularización** es una técnica importante en el aprendizaje automático que nos permite reducir la complejidad de nuestro modelo y evitar el sobreajuste. Consiste en aplicar una penalización a las variables menos relevantes del modelo.\n",
    "\n",
    "**¿Como hacemos lo anterior?**\n",
    "\n",
    "Para lograr esto, introducimos un **mayor sesgo** sobre las variables y disminuimos la varianza. De esta manera, mejoramos la capacidad de generalización del modelo y evitamos o reducimos el sobreajuste.\n",
    "\n",
    "<img src='https://www.andreaperlato.com/img/ridge.png'>\n",
    "\n",
    "En la imagen proporcionada, se muestra un ejemplo de sobreajuste. La línea azul se ajusta muy bien a los datos de prueba, pero no a los datos de entrenamiento. Esto resulta en una mala generalización y una aproximación deficiente.\n",
    "\n",
    "Para aplicar la regularización, necesitamos incluir un término adicional conocido como **función de pérdida (loss)**. La función de pérdida nos indica qué tan lejos están nuestras predicciones de los datos reales. En general, cuanto menor sea la pérdida, mejor será nuestro modelo.\n",
    "\n",
    "### Perdida en entrenamiento y en validación\n",
    "\n",
    "<img src='https://developers.google.com/static/machine-learning/crash-course/images/RegularizationTwoLossFunctions.svg' width=500>\n",
    "\n",
    "Podemos ver en la gráfica que la **pérdida tiende a disminuir**, porque en algún momento los datos de entrenamiento serán vistos y el modelo se ajustará a ellos. Sin embargo, lo importante es **cómo se comportará en el mundo real**.\n",
    "\n",
    "En el conjunto de validación o pruebas, es **normal que la pérdida comience a disminuir**, ya que indica una buena generalización. Sin embargo, llega un punto donde la introducción de nuevos valores hace que la pérdida vuelva a subir, lo cual generalmente se considera como **sobreajuste**. La pérdida es la medida que utilizamos para **aplicar la regularización**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tipos de regularización\n",
    "\n",
    "- **L1 (Lasso)**: Este regulizador reduce la complejidad del modelo eliminando características que tienen un impacto mínimo en la predicción. Al aplicar la regularización L1, algunos coeficientes del modelo se vuelven exactamente cero, lo que implica que estas características no son consideradas en la predicción final. Es útil para la selección automática de características relevantes.\n",
    "\n",
    "    $\\displaystyle\\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^{n}  \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} |\\beta|}$\n",
    "\n",
    "\n",
    "    Aquí está la descripción de cada componente de la fórmula:\n",
    "\n",
    "    - $\\displaystyle\\underset{\\beta}{\\text{arg min}}$: Se refiere al valor de los coeficientes $\\beta$ que minimiza la función de costo.\n",
    "\n",
    "    - $\\sum_{i=1}^{n}$: Representa la suma de los índices $i$ desde 1 hasta $n$, donde $n$ es el número de observaciones en el conjunto de datos.\n",
    "\n",
    "    - $y_i$: Es el valor real de la variable objetivo para la observación $i$.\n",
    "\n",
    "    - $\\beta_0$: Es el coeficiente de intersección o término de sesgo.\n",
    "\n",
    "    - $\\sum_{j=1}^{p}$: Representa la suma de los índices $j$ desde 1 hasta $p$, donde $p$ es el número de características o variables independientes en el conjunto de datos.\n",
    "\n",
    "    - $\\beta_j$: Son los coeficientes asociados a las características $x_{ij}$ del modelo.\n",
    "\n",
    "    - $x_{ij}$: Representa el valor de la característica $j$ para la observación $i$.\n",
    "\n",
    "    - $\\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} |\\beta|}$: Es la penalización que se aplica a los coeficientes. Se utiliza la norma L1 (valor absoluto) para sumar los coeficientes $\\beta_j$. El parámetro $\\lambda$ controla la intensidad de la penalización, donde un valor más alto de $\\lambda$ lleva a una mayor reducción de los coeficientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **L2 (Ridge)**: El regulizador L2 reduce la complejidad del modelo disminuyendo el impacto de ciertos features en la predicción. Penaliza los coeficientes grandes y favorece coeficientes más pequeños. A diferencia de L1, L2 no descarta características, sino que las reduce gradualmente. Es útil para evitar la multicolinealidad y mejorar la estabilidad del modelo.\n",
    "\n",
    "    $\\displaystyle\\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^{n}  \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} \\beta^2_{j}}$\n",
    "\n",
    "    Teniendo en cuenta que la descripción del regulador L2 (Ridge) contiene algunos puntos que se repiten con la descripción del regulador L1 (Lasso), los puntos que se pueden omitir en la descripción del regulador L2 son los siguientes:\n",
    "\n",
    "    - El uso de la función de costo cuadrática $(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2$ en lugar de la función de costo absoluta en la descripción del Lasso.\n",
    "\n",
    "    - $\\underset{\\text{penalización}}{\\lambda \\sum_{j=1}^{p} \\beta^2_{j}}$: Es la penalización que se aplica a los coeficientes. Se utiliza la norma L2 (cuadrados) para sumar los coeficientes $\\beta_j$ al cuadrado. El parámetro $\\lambda$ controla la intensidad de la penalización, donde un valor más alto de $\\lambda$ lleva a una mayor reducción de los coeficientes.\n",
    "\n",
    "- **ElasticNet**: Es una combinación de los regulizadores L1 y L2. Combina los efectos de selección automática de características de L1 con la regularización de coeficientes de L2. Proporciona un equilibrio entre la eliminación de características irrelevantes y la reducción del impacto de ciertos features en el modelo. Es útil cuando se sospecha que hay muchas características irrelevantes en los datos.\n",
    "\n",
    "   $\\displaystyle\\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^{n}  \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 +  \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2$\n",
    "\n",
    "   Donde:\n",
    "\n",
    "    - $(\\lambda_1)$ y $(\\lambda_2)$ son hiperparámetros que controlan la intensidad de las penalizaciones L1 y L2, respectivamente.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Lasso y Ridge\n",
    "\n",
    "Implementaremos las tecnicas de regularizacion. Para esto utilizaremos dos regresores que vienen por defecto en Scikit Learn y que de una manera automatizada nos integra un modelo lineal con su respectiva regularización.\n",
    "\n",
    "Utilizaremos el dataset del reporte de la felicidad mundial. Dataset que mide varios factores en diferentes paises, tales como, el indice de corrupción, nivel que nos indica que tan fuerte son las relaciones de familia, el desarrollo per capita económico y nos intenta dar una variable continua para medir la felicidad del pais en cuestión.\n",
    "\n",
    "Pasamos a crear un archivo con nombre ```regularization.py```\n",
    "\n",
    "1. Importar las bibliotecas necesarias:\n",
    "   - Se importa `pandas` para el manejo de datos estructurados.\n",
    "   - Se importan clases específicas de los módulos `sklearn.linear_model`, `sklearn.metrics` y `sklearn.model_selection` para su uso posterior.\n",
    "\n",
    "2. Leer el conjunto de datos:\n",
    "   - Se utiliza la función `pd.read_csv()` para leer un archivo CSV que contiene los datos del conjunto de datos.\n",
    "\n",
    "3. Separar características y variable objetivo:\n",
    "   - Se seleccionan las columnas específicas del conjunto de datos que se utilizarán como características (`X`) y se asignan a la variable `X`.\n",
    "   - Se selecciona la columna que se utilizará como la variable objetivo (`y`) y se asigna a la variable `y`.\n",
    "\n",
    "4. Dividir el conjunto de datos en conjuntos de entrenamiento y prueba:\n",
    "   - Se utiliza la función `train_test_split()` del módulo `sklearn.model_selection` para dividir los datos en conjuntos de entrenamiento (`X_train`, `y_train`) y prueba (`X_test`, `y_test`). Se especifica que el 25% de los datos se utilizarán como conjunto de prueba y se establece una semilla aleatoria (`random_state`) para la reproducibilidad.\n",
    "\n",
    "5. Entrenar los modelos de regresión:\n",
    "   - Se crean instancias de los modelos `LinearRegression`, `Lasso`, `Ridge` y `ElasticNet` del módulo `sklearn.linear_model`.\n",
    "   - Se ajustan (`fit()`) los modelos a los conjuntos de entrenamiento (`X_train`, `y_train`).\n",
    "\n",
    "6. Realizar predicciones en el conjunto de prueba:\n",
    "   - Se utilizan los modelos entrenados para hacer predicciones (`predict()`) en los conjuntos de prueba (`X_test`) y se guardan en las variables correspondientes (`y_predict_lineal`, `y_predict_lasso`, `y_predict_ridge`, `y_predict_elastic`).\n",
    "\n",
    "7. Calcular el MSE para cada modelo:\n",
    "   - Se utiliza la función `mean_squared_error()` del módulo `sklearn.metrics` para calcular el error cuadrático medio (MSE) entre las predicciones y los valores reales de las variables objetivo (`y_test`). Se calcula un MSE para cada modelo y se guarda en las variables correspondientes (`lineal_loss`, `lasso_loss`, `ridge_loss`, `elastic_loss`).\n",
    "\n",
    "8. Imprimir los valores de MSE para cada modelo:\n",
    "   - Se imprimen los valores de MSE calculados para cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación resultado de la implementación\n",
    "\n",
    "- **lineal_loss:**   9.893337283086869e-08\n",
    "- **lasso_loss:** 0.049605751139829145\n",
    "- **ridge_loss:** 0.005650124499962814\n",
    "- **elastic_loss:** 0.00912409728272294\n",
    "\n",
    "Menor perdida es mejor, esto quiere decir que hubo menos equivocacion entre los valores esperados y los valores predichos.\n",
    "\n",
    "|            | lineal   | lasso    | ridge    | elastic  |\n",
    "|------------|----------|----------|----------|----------|\n",
    "| gdp        | 1.000128 | 1.289214 | 1.072349 | 1.106542 |\n",
    "| family     | 0.999946 | 0.919694 | 0.970486 | 0.962883 |\n",
    "| lifexp     | 0.999835 | 0.476864 | 0.856054 | 0.803020 |\n",
    "| freedom    | 1.000034 | 0.732973 | 0.874002 | 0.861674 |\n",
    "| generosity | 1.000260 | 0.142455 | 0.732857 | 0.654667 |\n",
    "| corruption | 0.999771 | 0.000000 | 0.685833 | 0.554539 |\n",
    "| dystopia   | 0.999938 | 0.899653 | 0.962066 | 0.953720 |\n",
    "\n",
    "Los numeros mas grandes dentro del arreglo, significa que la columna en si esta teniendo mas peso en el modelo que estamos entrenando.\n",
    "\n",
    "- Lasso\n",
    "\n",
    "    Los valores que Lasso haya hecho 0, nos indica que el algoritmo no te dio la atencion necesaria o no los considero importante. Analizar porque hizo eso nuestro algoritmo Lasso ya esta tarea nuestra como Data Scientist.\n",
    "\n",
    "- Ridge\n",
    "\n",
    "    En Ridge ninguno de los coeficientes han sido 0, sino que fueron disminuidos, esto se hace precisamente la regresión Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones robustas\n",
    "\n",
    "## El problema de los valores atípicos\n",
    "\n",
    "\n",
    "### ¿Qué son los valores atípicos?\n",
    "\n",
    "- Un valor atípico es cualquier medición que se encuentre por fuera del comportamiento general de una muestra de datos.\n",
    "- Pueden indicar variabilidad, errores de medición o novedades.\n",
    "\n",
    "### ¿Por qué son problemáticos?\n",
    "\n",
    "- Pueden generar sesgos importantes en los modelos de Machine Learning.\n",
    "- A veces contienen información relevante sobre la naturaleza de los datos.\n",
    "- Detección temprana de fallos.\n",
    "\n",
    "### ¿Cómo identificarlos?\n",
    "\n",
    "A traves de métodos estadísticos\n",
    "- **Z-Score**: Mide la distancia (en desviaciones estándar) de un punto dado a la media.\n",
    "\n",
    "- Técnicas de clustering como **DBSCAN**.\n",
    "\n",
    "- Si $q<Q1 -1.5 * IQR\\;\\;$ ó $\\;\\;q>Q3+1.5 * IQR$\n",
    "\n",
    "A traves de métodos graficos\n",
    "\n",
    "- Boxplot \n",
    "\n",
    "    <img src='https://soka.gitlab.io/blog/post/2019-04-11-r-ggplot2-boxplot/images/boxplot-diagram-01.PNG' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresiones Robustas en Scikit-learn\n",
    "\n",
    "La regresión robusta es una técnica utilizada en análisis de regresión para mitigar los efectos de los valores atípicos o datos ruidosos en el ajuste del modelo. Los valores atípicos son observaciones que difieren significativamente del patrón general de los datos y pueden tener un impacto desproporcionado en los resultados de la regresión tradicional.\n",
    "\n",
    "Scikit Learn nos ofrece algunos modelos especificos para abordar el problema de los valores atipicos:\n",
    "\n",
    "### RANSAC\n",
    "**RANSAC** es un algoritmo que encuentra la **mejor aproximación de un modelo matemático en datos ruidosos o con valores atípicos**. Selecciona muestras aleatorias, ajusta el modelo y encuentra los **puntos cercanos al modelo dentro de un umbral de error**. Repite el proceso varias veces y elige el **modelo con más puntos cercanos (inliers) como el resultado final**. Es útil cuando los datos son problemáticos debido a **valores atípicos o ruido**.\n",
    "\n",
    "[RANSAC - 5 Minutes with Cyrill](https://www.youtube.com/watch?v=9D5rrtCC_E0)\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/c/c0/RANSAC_LINIE_Animiert.gif' width=500>\n",
    "\n",
    "\n",
    "### Huber Regressor\n",
    "\n",
    "El **Huber Regressor** es un algoritmo de regresión que trata de ser **resistente a valores atípicos** en los datos. En lugar de ignorarlos por completo, les asigna **menos influencia utilizando un parámetro llamado \"epsilon\"**. Los datos con errores absolutos por debajo de \"epsilon\" se tratan como **valores típicos y se usan en la función de pérdida cuadrática**, mientras que los datos con **errores mayores que \"epsilon\" se consideran valores atípicos y se tratan con la función de pérdida lineal**. El valor de **\"epsilon = 1.35\" se ha demostrado como una elección efectiva que logra un 95% de eficiencia estadística** en muchos casos. Esto permite que el modelo sea **robusto, proporcionando predicciones precisas incluso cuando existen valores atípicos en el conjunto de datos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para la regresión robusta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un archibo robust.py y a continuacion realizamos los siguientes pasos\n",
    "\n",
    "1. `import pandas as pd`: Importa la librería pandas con el alias \"pd\" para manejar y manipular los datos en formato de DataFrames.\n",
    "\n",
    "2. `from sklearn.linear_model import RANSACRegressor, HuberRegressor`: Importa los estimadores RANSACRegressor y HuberRegressor de la librería scikit-learn, que son algoritmos de regresión robustos.\n",
    "\n",
    "3. `from sklearn.svm import SVR`: Importa el estimador Support Vector Regression (SVR) de scikit-learn, que es un algoritmo de regresión basado en Máquinas de Soporte Vectorial.\n",
    "\n",
    "4. `from sklearn.model_selection import train_test_split`: Importa la función train_test_split de scikit-learn para dividir el conjunto de datos en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "5. `from sklearn.metrics import mean_squared_error`: Importa la función mean_squared_error de scikit-learn para calcular el error cuadrático medio.\n",
    "\n",
    "6. `dataset = pd.read_csv('./data/felicidad_corrupt.csv')`: Lee un archivo CSV llamado \"felicidad_corrupt.csv\" y carga los datos en un DataFrame llamado \"dataset\".\n",
    "\n",
    "7. `print(dataset.head(5))`: Muestra las primeras 5 filas del DataFrame para visualizar los datos.\n",
    "\n",
    "8. `X = dataset.drop(['country', 'score'], axis=1)`: Selecciona todas las columnas excepto \"country\" y \"score\" como características de entrada (variables independientes) y almacena el resultado en \"X\".\n",
    "\n",
    "9. `y = dataset.score`: Selecciona la columna \"score\" como la variable objetivo (variable dependiente) y la almacena en \"y\".\n",
    "\n",
    "10. `X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=42)`: Divide el conjunto de datos en conjuntos de entrenamiento y prueba. El 70% de los datos se utilizará para entrenar los modelos (conjuntos X_train e y_train), y el 30% restante se utilizará para evaluar el rendimiento de los modelos (conjuntos X_test e y_test).\n",
    "\n",
    "11. `estimadores = {...}`: Crea un diccionario llamado \"estimadores\" que contiene tres instancias de estimadores diferentes: SVR, RANSACRegressor y HuberRegressor. Cada estimador se inicializa con sus respectivos hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación regresión robusta\n",
    "\n",
    "1. `for name, estimador in estimadores.items():`: Itera sobre cada elemento del diccionario de estimadores, obteniendo el nombre del estimador como `name` y el estimador en sí como `estimador`.\n",
    "\n",
    "2. `estimador.fit(X_train, y_train)`: Entrena el estimador utilizando el conjunto de características de entrenamiento `X_train` y el vector de etiquetas de entrenamiento `y_train`.\n",
    "\n",
    "3. `predictions = estimador.predict(X_test)`: Realiza predicciones utilizando el estimador entrenado en el conjunto de características de prueba `X_test`, almacenando los resultados en el vector `predictions`.\n",
    "\n",
    "4. `print(\"=\"*32)`: Imprime una línea de 32 \"=\" para separar los resultados de diferentes estimadores.\n",
    "\n",
    "5. `print(name)`: Imprime el nombre del estimador actual.\n",
    "\n",
    "6. `print(\"mse:\", mean_squared_error(y_test, predictions))`: Calcula el error cuadrático medio (Mean Squared Error, MSE) entre las etiquetas de prueba `y_test` y las predicciones `predictions`, y lo imprime junto con el texto \"mse:\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de ensamble aplicados a clasificación\n",
    "\n",
    "## ¿Qué son los métodos de ensamble?\n",
    "\n",
    "Los métodos de ensamble son técnicas que combinan múltiples modelos de aprendizaje automático para obtener una predicción final más precisa y robusta. En lugar de confiar en un solo modelo, los métodos de ensamble aprovechan la diversidad y el poder predictivo de varios modelos para mejorar el rendimiento general.\n",
    "\n",
    "1. **Combinar diferentes métodos de ML con diferentes configuraciones y aplicar un método para lograr un consenso:** Los métodos de ensamble combinan modelos de aprendizaje automático variados y utilizan técnicas como el promedio o modelos \"meta\" para obtener una predicción más sólida.\n",
    "\n",
    "2. **La diversidad es una muy buena opción:** La diversidad entre los modelos base es crucial. Si los modelos son diversos y cometen errores diferentes, el ensamble puede corregir y mejorar el rendimiento general. Se logra mediante distintas técnicas de muestreo y configuración de modelos.\n",
    "\n",
    "3. **Los métodos de ensamble se han destacado por ganar muchas competencias de ML:** Estos métodos son altamente efectivos en competencias y desafíos de aprendizaje automático. Han ganado muchas competiciones de ciencia de datos gracias a su capacidad para mejorar la precisión y estabilidad de los modelos predictivos. Son ampliamente utilizados en aplicaciones reales para resolver diversos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el **aprendizaje automático**, los **métodos de ensamble** son **técnicas** que combinan **múltiples modelos** de aprendizaje para **mejorar la precisión y generalización** del modelo resultante. Dos **estrategias populares** de ensamble son el **\"bagging\"** y el **\"boosting\"**. Ambas estrategias utilizan el **concepto de entrenar varios modelos** y **combinar sus predicciones**, pero **difieren en cómo se generan y ponderan los modelos base**.\n",
    "\n",
    "\n",
    "### Bagging (Bootstrap Aggregating):\n",
    "\n",
    "**Bagging** es una **estrategia de ensamble** que se basa en el concepto de *bootstrap*. En este enfoque, se generan múltiples muestras de entrenamiento a partir del conjunto de datos original, cada una de ellas con reemplazo. Es decir, se toman muestras aleatorias del conjunto de datos original permitiendo que una misma muestra pueda aparecer múltiples veces y otras no aparezcan en absoluto en una determinada muestra.\n",
    "\n",
    "Después de generar las muestras, se entrena un **modelo base** (como un árbol de decisión o un modelo lineal) en cada una de ellas. Luego, las **predicciones** de cada modelo se combinan a través de **votación** (en el caso de clasificación) o **promediando** (en el caso de regresión) para obtener la predicción final del ensamble. Bagging ayuda a reducir la **varianza** y, por lo tanto, a mejorar la generalización del modelo, ya que los diferentes modelos base capturan diferentes aspectos del conjunto de datos.\n",
    "\n",
    "El algoritmo de bagging más conocido es el **\"Random Forest\"**, que utiliza árboles de decisión como modelos base y agrega sus predicciones para obtener una predicción más robusta y precisa. Además, tenemos otros algoritmos como el **\"Voting Classifiers/Regressors\"** y, en general, se puede aplicar bagging sobre cualquier familia de modelos de *machine learning*.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "**Boosting** es otra **estrategia de ensamble** que se enfoca en mejorar la precisión del modelo al dar más importancia a los ejemplos difíciles de clasificar. En lugar de entrenar múltiples modelos de manera independiente, el algoritmo de boosting **entrena secuencialmente** una serie de **modelos base**, cada uno de los cuales se construye para corregir los errores del modelo anterior.\n",
    "\n",
    "En cada iteración del proceso de boosting, los ejemplos mal clasificados por el modelo actual se **ponderan** más para que el siguiente modelo se enfoque en corregir esos errores. Este proceso continúa iterativamente hasta que se alcanza un cierto número de modelos o cuando se logra un criterio de detención predefinido. Luego, las **predicciones** de cada modelo base se combinan mediante una ponderación basada en su rendimiento, para obtener la predicción final del ensamble.\n",
    "\n",
    "El algoritmo de boosting más conocido es **\"AdaBoost\"** (Adaptive Boosting), que generalmente utiliza árboles de decisión como modelos base, pero también se puede combinar con otros algoritmos de aprendizaje. Otros algoritmos pueden ser **Gradient Tree Boosting** y **XGBoost**.\n",
    "\n",
    "**Boosting** tiende a tener un mejor rendimiento que bagging cuando los modelos base son suficientemente fuertes y se combinan bien con ellos.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/768px-Ensemble_Boosting.svg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos para implementar métodos de ensamble\n",
    "\n",
    "Utilizaremos un meta estimador que tiene Scikit Learn llamado Bagging Classifier. Al ser un meta estimador podemos adaptarlo a las diferentes familias de estimadores y Scikit Learn lo configurara de forma automática para que se convierta en un método de ensamble.\n",
    "\n",
    "Utilizaremos el dataset de afecciones cardiacas. Teniamos diferentes datos de pacientes, donde la meta finalmente era clasificarlos en si el paciente tenia o no una afección cardiaca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementación de Bagging\n",
    "Este código es un ejemplo de cómo usar el ensamble de Bagging con varios clasificadores en un conjunto de datos de clasificación binaria (columna 'target' con etiquetas 0 y 1) utilizando la biblioteca scikit-learn de Python.\n",
    "\n",
    "```Creamos un archivo bagging.py```\n",
    "\n",
    "Pasos del código:\n",
    "\n",
    "1. Se importan las bibliotecas necesarias, como pandas para trabajar con el conjunto de datos, varios clasificadores (KNeighborsClassifier, LinearSVC, SVC, SGDClassifier y DecisionTreeClassifier) y la clase BaggingClassifier para realizar el ensamble de Bagging.\n",
    "\n",
    "2. Se lee el conjunto de datos desde un archivo CSV ('heart.csv') utilizando pandas y se muestra el contenido del DataFrame 'df_heart' y la descripción estadística de la columna objetivo 'target'.\n",
    "\n",
    "3. Se separa el conjunto de datos en atributos (X) y etiquetas (y). Los atributos son todas las columnas excepto la columna objetivo, mientras que las etiquetas son la columna objetivo.\n",
    "\n",
    "4. Se divide el conjunto de datos en conjuntos de entrenamiento y prueba utilizando la función train_test_split de scikit-learn.\n",
    "\n",
    "5. Se define un diccionario llamado 'classifier' que contiene varios clasificadores junto con sus nombres como claves.\n",
    "\n",
    "6. Se itera sobre cada clasificador del diccionario y se realiza lo siguiente:\n",
    "   a. Se ajusta el modelo con el conjunto de entrenamiento (X_train, y_train).\n",
    "   b. Se hacen predicciones en el conjunto de prueba (X_test) y se calcula la precisión del modelo utilizando accuracy_score de scikit-learn.\n",
    "   c. Luego, se crea un clasificador Bagging utilizando el modelo base actual como 'base_estimator' y se ajusta con el conjunto de entrenamiento (X_train, y_train) utilizando BaggingClassifier.\n",
    "   d. Se hacen predicciones en el conjunto de prueba (X_test) con el clasificador Bagging y se calcula la precisión nuevamente.\n",
    "\n",
    "7. Se imprime la precisión de cada clasificador y su respectivo clasificador Bagging en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Boosting\n",
    "\n",
    "Para realizar el ejemplo con **boosting**, se agregó la biblioteca **GradientBoostingClassifier** de scikit-learn. Luego, se **ajustó el modelo GradientBoostingClassifier** con el conjunto de entrenamiento *(X_train, y_train)* y se **hicieron predicciones** en el conjunto de prueba *(X_test)* utilizando este modelo. Se **calculó la precisión** de las predicciones utilizando **accuracy_score** y se **imprimió el resultado** para cada clasificador, indicando que se trata del **Gradient Boosting Classifier**. De esta manera, se pudo comparar la precisión del ensamble de boosting con la de los otros clasificadores y sus versiones bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "## Estrategias de Clustering\n",
    "\n",
    "Los algoritmos de clustering son las estrategias que podemos usar para agrupar los datos de tal manera que todos los datos pertenecientes a un grupo sean lo mas similiares que sea posible entre si, y mas diferentes a los otros grupos.\n",
    "\n",
    "### Casos de uso\n",
    "\n",
    "1. **No conocemos con anterioridad las etiquetas de nuestros datos (Aprendizaje no supervisado):** En el aprendizaje no supervisado, no se proporcionan etiquetas o categorías para los datos de entrenamiento. En lugar de eso, el algoritmo de clustering agrupa los datos en función de sus características y similitudes, sin que se le indique previamente qué categorías deberían formarse. Es útil cuando se desconoce la estructura intrínseca de los datos o cuando se buscan agrupamientos naturales sin información de referencia.\n",
    "\n",
    "2. **Queremos descubrir patrones ocultos a simple vista:** Los algoritmos de clustering ayudan a identificar patrones o estructuras ocultas en los datos que pueden no ser evidentes a simple vista. Al agrupar los datos en clusters, podemos visualizar y entender mejor la distribución y las relaciones entre los elementos de los datos, lo que puede conducir a un mayor conocimiento y comprensión del conjunto de datos.\n",
    "\n",
    "3. **Queremos identificar datos atípicos:** Los datos atípicos (outliers) son puntos que se encuentran lejos del patrón general de los datos y pueden tener un comportamiento inusual o significativo. El clustering puede ayudar a identificar estos datos atípicos al agrupar los puntos en clusters y observar aquellos puntos que no se ajustan bien a ningún grupo específico. Detectar datos atípicos es valioso en muchas aplicaciones, como el análisis de fraude, control de calidad o diagnóstico médico, donde es esencial identificar valores inusuales que puedan indicar problemas o condiciones especiales.\n",
    "\n",
    "### Dos casos de aplicación\n",
    "1. **Cuando sabemos cuántos grupos *k* queremos:**\n",
    "En este caso, conocemos previamente el número de grupos (*k*) que deseamos obtener en el resultado del clustering.\n",
    "\n",
    "    En estos casos se recomiendan usar ***k-means***, o bien, ***Spectral Clustering***\n",
    "\n",
    "2. **Cuando queremos que el algoritmo determine el número óptimo de grupos *k*:**\n",
    "Aquí no tenemos información sobre la cantidad adecuada de grupos, y buscamos que el algoritmo descubra automáticamente el número óptimo de grupos basado en los datos.\n",
    "\n",
    "    En estos casos usaremos ***Meanshift, Herarchical Clustering***, o bien ***DBScan***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Batch K-Means\n",
    "\n",
    "Asumiremos que sabemos los grupos que implementariemos en el resultado final\n",
    "\n",
    "Utilizaremos el dataset de candy. Este nos dice las caracterisicas de diferentes caramelos. Podemos conocer mas el dataset en el su readme.\n",
    "\n",
    "Usamos la implementación Mini Batch K-Means. Esta es una variante del algoritmo K-Means que usa mini batches (lotes) que reduce el tiempo de computo. La unica diferencia es que la calidad de los resultados es reducida.\n",
    "\n",
    "[sklearn.cluster.MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)\n",
    "\n",
    "No implementamos el metodo del codo, al conocerce el datasets utilizamos la cantidad de cluster adecuada, aunque el metodo de seleccion de estos no fue el adecuado, ya que debe usarse el metodo de codo u otro.\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/315702ef4aa54369a673dc7ab67a14d0c44027117bb8174b1e6e3ba12d4aabbc/68747470733a2f2f696d6775722e636f6d2f4b3944715545382e706e67' width=500>\n",
    "\n",
    "- sugarpercent: ​ El percentil de azúcar en el que recae dentro del mismo dataset.\n",
    "- pricepercent: El percentil de precio por unidad dentro del que se encuentra respecto al dataset.\n",
    "- winpercent:​ Porcentaje de victorias de acuerdo a 269.000 emparejamientos al azar.\n",
    "\n",
    "> Se observan los 4 diferentes colores, ya que se eligieron 4 clusters. Se puede observar una clara clusterizacion cuando se compara respecto a winpercent las variables pricepercent, sugarpercent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementactión de Mean-Shift\n",
    "\n",
    "Puede suceder que lo que necesitemos sea simplemente dejar que el algoritmo decida cuantas categorías requiere. Esto lo podremos hacer con el algoritmo [Mean-Shift](https://scikit-learn.org/stable/modules/clustering.html#mean-shift). El algoritmo de la clustering tiene como objetivo descubrir manchas en una densidad uniforme de muestras. O sea, diferenciar y clusterizar.\n",
    "\n",
    "[Sklearn.cluster.MeanShift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html)\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/7a4eb7221655350c71faa67612a79cf37ca7f254c77ff99ba4b7419874aa5271/68747470733a2f2f696d6775722e636f6d2f684e75503256592e706e67' width=500>\n",
    "\n",
    "El algoritmo nos devolvio 3 clusters, porque le pareció que esa era la cantidad correcta teniendo en cuenta como se distrubuye la densidad de nuestros datos. Podemos ver eso mismo en el gráfico anterior.\n",
    "\n",
    "Se observan los 3 diferentes colores, clusters generados automaticamente por el algoritmo MeanShift. Se puede observar una clara clusterizacion cuando se compara respecto a winpercent las variables pricepercent, sugarpercent.\n",
    "\n",
    "NOTA: En la documentación (en Scalability) se advierte que el algoritmo tiene una complejidad algorítmica que escala a O(T*n^2) a medida que el número de registros aumenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización paramétrica\n",
    "\n",
    "Hasta ahora hemos estudiado y hecho:\n",
    "\n",
    "- Aprender a lidiar con Features antes de mandarlo al entrenamiento.\n",
    "- Aprender modelo espeficidos para resolver problemas de gran complejidad.\n",
    "\n",
    "Ahora nos toca la etapa final del proceso de Machine Learning, esto es:\n",
    "\n",
    "- Validacion de lo que se ha hecho. Scikit Learn nos ofrece realizar este tipo de tareas de una manera casi automatizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación de nuestros modelos. Hold Out y Cross Validation\n",
    "\n",
    "- La última palabra siempre la van a tener los datos.\n",
    " \n",
    "    - Todas nuestras intuiciones no tiene nada que hacer frente a lo que digan los datos y las matemáticas que aplicamos sobre estos datos. Por eso es importante siempre tener rigurosidad a la hora de evaluar los resultados que estamos recibiendo.\n",
    "- Necesitamos mentalidad de testeo.\n",
    " \n",
    "- No se trata solamente de probar un poco al principio y un poco al final, sino que tendremos que probar constantemente durante todo el proceso, para poder encontrar cuál es la solución óptima que realmente nos soluciona el problema que tenemos pendiente, todo esto:\n",
    "    - con varias formas\n",
    "        - con varios conjuntos de datos\n",
    "        - con varias configuraciones de parámetros\n",
    "        - con varias distribuciones de nuestros datos\n",
    "        - Todos los modelos son malos, solamente algunos son útiles.\n",
    " \n",
    "- Todos los modelos que nosotros hacemos en últimas son una sobre simplificación de lo que pasa realmente. Entonces nunca nuestros modelos van a corresponder con la realidad al cien por ciento. Si jugamos lo suficiente y si somos lo suficientemente hábiles para configurar, vamos a llegar a un punto donde el modelo que estamos trabajando va a ser útil para ciertos casos específicos dentro del mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de validaciones\n",
    "\n",
    "**Hold-Out:**\n",
    "\n",
    "La validación Hold-Out es uno de los métodos más sencillos de validación cruzada. Implica dividir el conjunto de datos en dos partes: el conjunto de entrenamiento (train) y el conjunto de prueba (test). Por lo general, se asigna una parte mayor de los datos al conjunto de entrenamiento y una parte más pequeña al conjunto de prueba. Este método es adecuado en situaciones donde se necesita un prototipado rápido, hay poco conocimiento en Machine Learning y recursos limitados en términos de poder de cómputo. También puede ser útil cuando se quiere una separación clara entre los datos de entrenamiento y prueba para evitar fugas de información.\n",
    "\n",
    "¿Cuándo utilizar Hold-on?\n",
    "\n",
    "- Se requiere un prototipado rápido.\n",
    "- No se tiene mucho conocimiento en ML.\n",
    "- No se cuenta con abundante poder de cómputo.\n",
    "\n",
    "<img src='https://vitalflux.com/wp-content/uploads/2020/12/Hold-out-method-for-model-evaluation.png' width=400>\n",
    "<img src='https://vitalflux.com/wp-content/uploads/2020/12/Hold-out-method-Training-Validation-Test-Dataset.png' width=400>\n",
    "\n",
    "**K-Folds:**\n",
    "\n",
    "K-Folds es un enfoque más robusto que el Hold-Out. En este método, el conjunto de datos se divide en K partes o \"pliegues\" (folds) de aproximadamente igual tamaño. Luego, el modelo se entrena y evalúa K veces, cada vez utilizando un fold diferente como conjunto de prueba y el resto de los pliegues como conjunto de entrenamiento. Este método es recomendable en la mayoría de los casos, especialmente cuando se tiene un equipo adecuado para desarrollar Machine Learning, ya que permite aprovechar mejor los datos disponibles. También es útil cuando se requiere integrar técnicas de optimización paramétrica, como la búsqueda de hiperparámetros, ya que se pueden evaluar diferentes configuraciones de manera más robusta. K-Folds es más adecuado cuando se dispone de tiempo para realizar múltiples pruebas.\n",
    "\n",
    "¿Cuándo utilizar K-Folds?\n",
    "\n",
    "- Recomendable en la mayoría de los casos.\n",
    "- Se cuenta con un equipo suficiente para desarrollar ML.\n",
    "- Se require la integración con técnicas de optimización paramétrica.\n",
    "- Se tiene más tiempo para las pruebas.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png'>\n",
    "\n",
    "**LOOCV (Leave-One-Out Cross-Validation):**\n",
    "\n",
    "LOOCV es un enfoque de validación cruzada extremadamente riguroso. En LOOCV, se entrena el modelo en K-1 pliegues y se evalúa en un pliegue de prueba diferente en cada iteración, donde K es igual al número total de muestras en el conjunto de datos. Este enfoque es ideal cuando se dispone de un gran poder de cómputo, ya que realiza una validación exhaustiva probando cada muestra como conjunto de prueba en algún momento. LOOCV es útil cuando se tienen muy pocos datos para dividir en conjuntos de entrenamiento y prueba, ya que garantiza que cada punto de datos se use tanto para entrenamiento como para prueba. Sin embargo, puede ser computacionalmente costoso y menos práctico en conjuntos de datos grandes debido a la cantidad masiva de iteraciones.\n",
    "\n",
    "<img src='https://biol607.github.io/lectures/images/cv/loocv.png' width=500>\n",
    "\n",
    "¿Cuándo utilizar LOOCV?\n",
    "\n",
    "- Se tiene gran poder de computo\n",
    "- Se cuetan con pocos datos para poder dividir por Train/Test\n",
    "- Cuando se quiere probar todos los casos posibles (para personas con TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de K-Folds Cross Validation\n",
    "\n",
    "Este código es un ejemplo de cómo se podría realizar la validación cruzada utilizando el algoritmo de regresión de árbol de decisión en Python con la librería Scikit-Learn. El objetivo principal es evaluar el rendimiento del modelo de regresión utilizando diferentes enfoques de validación cruzada y calcular el error cuadrático medio (MSE) en cada caso.\n",
    "\n",
    "A continuación, te proporciono un análisis línea por línea del código:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "```\n",
    "En esta sección, se importan las librerías necesarias, incluyendo pandas para el manejo de datos, numpy para cálculos numéricos, DecisionTreeRegressor para el modelo de regresión de árbol de decisión, cross_val_score y KFold para realizar validación cruzada, y mean_squared_error para calcular el error cuadrático medio.\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('./data/felicidad.csv')\n",
    "\n",
    "    X = data.drop(['country','score'],axis=1)\n",
    "    y = data.score\n",
    "```\n",
    "Esta parte del código verifica si el script se está ejecutando como el programa principal. Luego, lee un archivo CSV que supuestamente contiene datos de felicidad. A continuación, separa los datos en la matriz X (características) y el vector y (objetivo) eliminando la columna 'country' y 'score' de los datos originales.\n",
    "\n",
    "```python\n",
    "    model = DecisionTreeRegressor()\n",
    "    score = cross_val_score(model, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "```\n",
    "Aquí se crea una instancia de DecisionTreeRegressor como el modelo para regresión. Luego, se realiza una validación cruzada utilizando cross_val_score con 3 particiones (cv=3) y se utiliza el negativo del MSE como métrica de evaluación (scoring='neg_mean_squared_error'). El resultado es una lista de valores de MSE negativos para cada partición.\n",
    "\n",
    "```python\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_values = []\n",
    "    for train, test in kf.split(data):\n",
    "        X_train = X.iloc[train]\n",
    "        y_train = y.iloc[train]\n",
    "        X_test = X.iloc[test]\n",
    "        y_test = y.iloc[test]\n",
    "\n",
    "        model = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "        predict = model.predict(X_test)\n",
    "        mse_values.append(mean_squared_error(y_test, predict))\n",
    "```\n",
    "Se define un objeto KFold con 5 particiones, aleatorización (shuffle) y una semilla aleatoria fija (random_state=42). Luego, se realiza un bucle que itera a través de las particiones generadas por KFold. En cada iteración, se separan los datos en conjuntos de entrenamiento y prueba. Se ajusta un modelo de regresión de árbol de decisión utilizando los datos de entrenamiento y se realiza una predicción en los datos de prueba. El MSE de cada iteración se calcula utilizando la función mean_squared_error y se agrega a la lista mse_values.\n",
    "\n",
    "```python\n",
    "    print(\"Los tres MSE fueron: \", mse_values)\n",
    "    print(\"El MSE promedio fue: \", np.mean(mse_values))\n",
    "```\n",
    "Finalmente, se imprimen los valores de MSE calculados en cada iteración y el MSE promedio obtenido de todas las particiones de KFold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización paramétrica\n",
    "\n",
    "Familiarizados con el concepto de Cross Validation vamos a utilizar este mismo principio de fondo para lograr automatizar un poco la selección y optimización de nuestros modelos.\n",
    "\n",
    "Problema: Parece que encontramos un modelo de aprendizaje que parece funcionar, pero esto puede implicar que ahora tenemos que encontrar la optimización de cada uno de los parámetros de este modelo, encontrar el que mejor se ajuste y el que mejor resultado nos de.\n",
    "\n",
    "- Es facil perderse entre los conceptos de tantos parámetros. Tenemos flexibilidad para algoritmos básicos de Machine Learning, pero facil perderse.\n",
    "- Es difícil medir la sensibilidad de los mismos manualmente.\n",
    "- Es COSTOSO, en tiempo humano y computacionalmente.\n",
    "\n",
    "Scikit Learn nos ofrece enfoques para automatizar el proceso de optimización paramétrica. Existen 3 enfoques principales, estos son:\n",
    "\n",
    "- Optimización manual\n",
    "- Optimizacion por grilla de parámetros | GridSearchCV\n",
    "- Optimizacion por búsqueda aleatorizada | RandomizedSearchCV\n",
    "\n",
    "### Optimización manual\n",
    "- Escoger el modelo que queremos ajustar.\n",
    "- Buscar en la documentación de Scikit-Learn\n",
    "- Identificar parámetros y ajustes. Parámetros que vamos a necesitar y cuáles son los posibles ajustes que vamos a requerir para cada uno de estos parámetros.\n",
    "- Probar combinaciones una por una iterando a través de listas.\n",
    "\n",
    "### Optimizacion por grilla de parámetros | GridSearchCV\n",
    "Es una forma organizada, exhaustiva y sistematica de probar todos los parametros que le digamos que tenga que probar, con los respectivos rangos de valores que le aportemos.\n",
    "\n",
    "- Definir una o varias métricas que queremos optimizar.\n",
    "- Identificar los posibles valores que pueden tener los parámetros.\n",
    "- Crear un diccionario de parámetros.\n",
    "- Usar Cross Validation.\n",
    "- Entrenar el modelo (e ir por un café)\n",
    "\n",
    "La grilla de parámetros nos define GRUPOS DE PARÁMETROS que serán probados en todas sus combinaciones (Un grupo a la vez)\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/6ff7cea324f1524ed97c19db5884d58e019312dbe97e5afccb385ed8929704fd/68747470733a2f2f696d6775722e636f6d2f536467537570762e706e67' width=500>\n",
    "\n",
    "#### Optimizacion por búsqueda aleatorizada | RandomizedSearchCV\n",
    "Si no tenemos tanto tiempo para una prueba tan exhaustiva o queremos combinaciones aleatorias usaremos este metodo. Es lo mismo que el caso anterior, pero busca de forma aleatoria los parametros y Scikit Learn selecciona los mejores de las combinaciones aleatorias que se hicieron.\n",
    "\n",
    "En este método, definimos escalas de valores para cada uno de los parámetros seleccionados, el sistema probará varias iteraciones (Configurables según los recursos) y mostrará la mejor combinación encontrada.\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/165343f9952c2562d4c8db8e3cc63f74322ada8e66146ae8d6790974aa01f87f/68747470733a2f2f696d6775722e636f6d2f437243466c33572e706e67' width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salida a producción\n",
    "\n",
    "## Revisión de nuestra arquitectura de código\n",
    "\n",
    "Ahora vamos a convertir los scripts que tenemos en un código que sea modular y extensible con facilidad para que nuestra arquitectura pueda salir a producción de una manera exitosa.\n",
    "\n",
    "Una estructura de carpetas que sea organizada para poder gestionar todo lo que vas a necesitar en cualquier proceso de Machine Learning.\n",
    "\n",
    "**Carpetas:**\n",
    "\n",
    "- in: Carpeta que contendrá archivos de entrada, datos que alimentarán a nuestros modelos.\n",
    "- out: Carpeta que contendrá el resultado de la exportacion de nuestros modelos, visualizaciones, datos en excel o csv, etc.\n",
    "- models: Carpeta que contedrá a los modelos.\n",
    "\n",
    "**Archivos:** Cada clase será un archivo que tenga su propia responsabilidad y se encargue específicamente de una tareas concreta.\n",
    "\n",
    "- main.py: Metodo principal de ejecucion. Ejecutará todo el flujo de datos. Se encargaría de controlar el flujo de todo el código de principio a fin.\n",
    "- load.py: Archivo que se encarga de cargar los datos desde in o una DB\n",
    "- utils.py: Todos los metodos que se reutilizaran una y otra vez.\n",
    "- models.py: Irá toda la parte de ML como tal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar y exportar modelos con Sklearn\n",
    "\n",
    "Se creo la clase Models que contiene:\n",
    "\n",
    "models.py\n",
    "\n",
    "- Metodo grid_training(): Metodo para seleccionar al mejor modelo con el mejor score. Trabaja sobre los atributos, que son diccionarios de modelos y sus respectivos rangos y opciones de parámetros. Se utiliza el optimizador Grid y se selecciona finalmente el mejor modelo y el que mas score entrega de estos.\n",
    "- Atributos:\n",
    "    - reg: Atributo que contiene a los regresores en diccionarios. Estos son los modelos que se utilizarán\n",
    "    - params: Atributo que contiene a los parámetros de cada modelo en diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de una API con Flask para el modelo\n",
    "- Instalamos flask\n",
    "- Eliminamos load.py, ya que no lo utilizaremos finalmente\n",
    "- Creamos el archivo server.py para crear un servidor local para nuestra API.\n",
    "    - En este creamos la funcion predict(), que será la expuesta en nuestro servidor con el metodo GET, en la direccion 8080/predict y que muestra la prediccion hecha. La prediccion se hace con datos de pruebas y con nuestro modelo que exportamos al archivo best_model.pkl\n",
    "\n",
    "Tenemos entonces un JSON que tiene una llave que se llama predicción y el valor de la predicción que nos generó nuestro modelo según los datos que le pasamos de configuración.\n",
    "\n",
    "```\n",
    "http://127.0.0.1:8080/predict\n",
    "```\n",
    "\n",
    "sí podemos entonces ver un ejemplo de cómo podríamos salir a producción.\n",
    "\n",
    "Ya el JSON tendríamos que tratarlo, si estamos desarrollando una aplicación móvil o una plataforma web, podríamos trabajarlo con JavaScript o desde Android sin importar la naturaleza lo que estemos haciendo.\n",
    "\n",
    "Con esto ya tenemos las predicciones y tenemos un sistema que se conecta a nuestro modelo y nos trae los resultados de una manera extensible, modular, fácil de utilizar y que podemos convertir en la solución que estamos buscando.\n",
    "\n",
    "Así damos por finalizado la construcción de la arquitectura para salir a producción de nuestro modelo Inteligencia artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "## Manejo de features\n",
    "Optimización de features\n",
    "\n",
    "En el curso aprendimos cómo tratar con nuestro features y como seleccionarlos para extraer la información más importante. Esto es optimización de features a través de PCA, IPCA, KPCA. Tambienn Regularización e implementación de Lasso y Ridge\n",
    "\n",
    "## Algoritmos de ML\n",
    "También como construir algunos modelos de Machine Learning aún para casos bastante complejos como los que vimos.\n",
    "\n",
    "Nos adentramos en las tres areas de Machine Learning mas importantes como son:\n",
    "\n",
    "Regresiones robustas: Estudiamos sobre Regresiones robustas y como implementarlas para evitar valores atípicos.\n",
    "\n",
    "Métodos de ensamble aplicados a clasificación: Estudiamos métodos de ensamble aplicados a clasificación, preparamos datos e implementamos Bagging y Boosting.\n",
    "\n",
    "Clustering: Estudiamos estrategias de Clustering y como implementar Batch K-Means y Mean-Shift\n",
    "\n",
    "## Validacion y optimizacion de hiperparametros\n",
    "Optimización paramétrica\n",
    "\n",
    "Se le dedico un modulo completo a como validar nuestros modelos. Conocimos en profundiad los tipos de validación (Hold-Out, K-Folds, LOOCV). Esto se lo conoce como Cross Validation.\n",
    "\n",
    "Luego en el mismo modulo conocimos y estudiamos sobre Optimización paramétrica o Hyperparameter Optimization. Implementamos GridSearchCV y RandomizedSearchCV\n",
    "\n",
    "## Como exponer un modelo en produccion\n",
    "Salida a producción\n",
    "\n",
    "Finalmente cómo sacarlos a producción a través de una APIrest\n",
    "\n",
    "Formamos una arquitectura de archivos y carpetas para nuestro código, importar y exportar modelos con Sklearn y creamos una APIrest con Flask para el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
